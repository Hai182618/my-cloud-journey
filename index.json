[{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.1-event1/","title":"Event Participation – AI/ML/GenAI on AWS","tags":[],"description":"","content":"Event 1 – AI/ML/GenAI on AWS Event Name: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 8:30 AM – 12:00 PM, November 15, 2025\nLocation: AWS Vietnam Office\nRole: Attendee (Student)\nDescription of Event Activities This half-day workshop provided a comprehensive introduction to modern AI/ML and Generative AI workloads on AWS. The event was structured into multiple sessions, starting with participant registration, welcome activities, and an overview of the AI/ML landscape in Vietnam. The session also included an ice-breaker and orientation on workshop objectives.\nThe first main segment focused on AWS AI/ML services, with deep dives into:\nAmazon SageMaker and its end-to-end machine learning workflow Data preparation and labeling Model training, tuning, deployment Integrated MLOps features A live demo using SageMaker Studio After a short break, the workshop continued with an extensive session on Generative AI using Amazon Bedrock, covering:\nBedrock-supported Foundation Models (Claude, Llama, Titan) and selection considerations Prompt Engineering techniques such as CoT and few-shot prompting Retrieval-Augmented Generation (RAG) and knowledge base integration Bedrock Agents and multi-step orchestration Guardrails for safety and filtering A live demo building a GenAI chatbot Participation \u0026amp; Role As an attendee, I actively listened to the presentations, followed the demos, observed workflows, and took mental notes on the technologies being demonstrated. Although I did not directly practice hands-on labs, I gained valuable exposure to real-world AI/ML tools and architectures used by AWS.\nOutcomes \u0026amp; Value Gained From this event, I gained:\nA broader view of how AI/ML and GenAI are being applied in Vietnam A clearer understanding of AWS’s machine learning stack, especially SageMaker workflows Practical knowledge of modern GenAI techniques through Amazon Bedrock Insights into model selection, RAG design, prompt engineering, and safety guardrails Greater confidence and motivation to pursue Cloud \u0026amp; AI/ML as a career direction This event significantly reinforced my cloud knowledge foundation and helped me connect theoretical concepts with real-world AI/ML workflows.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Understand the fundamentals of Module 01 – Cloud Computing. Learn AWS global infrastructure, management tools, cost optimization, and AWS Support. Tasks to be carried out this week: Day Task Start Date Completion Date 1 - Introduction \u0026amp; orientation - Module 01 – What is Cloud Computing? 08/09/2025 08/09/2025 2 Module 01 – What makes AWS different? 09/09/2025 09/09/2025 3 Module 01 – How to start your cloud journey? 10/09/2025 10/09/2025 4 Module 01 – AWS Global Infrastructure 11/09/2025 11/09/2025 5 Module 01 – AWS Service Management Tools 12/09/2025 12/09/2025 6 Module 01 – Cost Optimization on AWS Module 01 – Working with AWS Support 13/09/2025 13/09/2025 7 Module 01 – Hands-on practice \u0026amp; additional research - Module 01 Summary 14/09/2025 14/09/2025 Week 1 Achievements: Gained a solid foundational understanding of Cloud Computing, including its core characteristics, service models (IaaS, PaaS, SaaS), and reasons companies migrate to AWS.\nUnderstood key differentiators of AWS: scalability, flexibility, global coverage, security posture, high availability, and rapid innovation.\nLearned how to begin a cloud adoption journey—assessment, migration planning, service selection, and the concept of landing zones.\nMastered AWS Global Infrastructure components: Regions, Availability Zones, Edge Locations, Local Zones, and Wavelength Zones, and how they influence architectural decisions.\nBecame proficient in AWS service management tools:\nAWS Management Console AWS CLI AWS CloudShell AWS Documentation AWS Pricing Calculator Understood AWS cost optimization: instance selection, Auto Scaling, Reserved Instances, Savings Plans, and Cost Explorer monitoring.\nLearned how to work effectively with AWS Support: support plans, benefits, case creation, and issue tracking.\nCompleted hands-on labs that reinforced Module 01 concepts through navigation, scenario exploration, and foundational cloud operations.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Video Encoding on Graviton in 2025 In 2022, we published a post describing the advantages of running video encoding workloads on AWS Graviton processors. Since that time, AWS launched Graviton4 powered C8g instances which offer up to 30% better performance than Graviton3. On video encoding workloads, Graviton4 performs 12–15% better than Graviton3, depending on the encoder.\nPerformance Improvements on x265 Figure 1. Percent improvement comparing C8g vs C7g. Contributions to x265 improved performance across encoding presets. Some of the most time-consuming functions—SAD, convolution, DCT—were optimized, resulting in significant gains. Graviton4, with SVE2, shows the strongest improvements.\nThese graphs show the improvement of x265 v4.1 over 3.6. From 3.5 to 4.1, performance increased 2.3–2.5×.\nFigure 2. Price-performance comparison for FFmpeg workloads. Tips to Maximize Performance on Graviton Build from latest source (x265 and SVT-AV1 improve frequently). Use the newest compiler (Clang-17+ recommended). Prefer Clang over GCC — up to 11% improvement on C8g. Figure 3. Clang-18 vs GCC-13 performance on C8g. New Instruction Support Enables More Optimizations Graviton3 introduced SVE; Graviton4 added SVE2, simplifying SIMD code and enabling new optimizations.\nExample:\nThe function saoCuStatsE0 in x265 gains:\n2.7× faster with NEON 3.2× faster with SVE2 This is achieved by replacing five compare + five SADALP operations with a single histseg instruction.\nHDR and 10-bit Video 10/12-bit content requires twice the SIMD compute of 8-bit. Major progress has been made for x265 and SVT-AV1:\nC8g: +12% C7g: +8% C6g: +10% Figure 4. 10-bit encoding improvements across Graviton generations. Benchmarking Method Workloads fully loaded all cores. Used raw Y4M input encoded with x264/x265/SVT-AV1. FPS aggregated across all processes. Conclusion Graviton4 provides:\n12% faster than Graviton3 73% faster than Graviton2 To maximize performance:\nUse the latest encoder builds Prefer Clang Monitor contributions to x265 \u0026amp; SVT-AV1 Author: Jonathan Swinney – AWS (Annapurna Labs)\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create a gateway endpoint","tags":[],"description":"","content":" Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://thienluhoan.github.io/workshop-template/","title":"Internship Report","tags":[],"description":"","content":"Internship Report ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Nguyen Minh Hai\nPhone Number: 0968974611\nEmail: Hainmse182618@fpt.edu.vn\nUniversity: FPT University\nMajor: System Information\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-Evaluation Report Feedback "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/","title":"Worklog","tags":[],"description":"","content":"This page provides an overview of my 12-week worklog throughout the internship program.\nIt describes how I completed the learning journey, what weekly objectives I followed, and how each activity contributed to my technical growth in AWS cloud engineering.\nOver the span of approximately three months, I documented weekly progress covering foundational AWS knowledge, hands-on cloud computing skills, operational best practices, and exploration of new-generation AWS technologies such as Graviton4 performance tuning, Amazon Bedrock, and Amazon Q Developer plugins.\n⭐ Structure of My Worklog My worklog follows a weekly structure. Each week includes:\nA specific learning objective Hands-on tasks and experiments Reflection on AWS services used Application of concepts to real-world scenarios Below is the weekly outline of my 12-week learning journey:\nWeek 1: Getting familiar with AWS and foundational services Introduction to AWS console, IAM, VPC, EC2, S3, CloudWatch. Setting up a stable working environment.\nWeek 2: Working with compute workloads \u0026amp; Graviton instances Explored AWS Graviton2/3/4 architectures and benchmark behavior based on Blog 1 (Video Encoding on Graviton). Understood SIMD, SVE2, x265 optimization insights.\nWeek 3: Hands-on: Video encoding performance analysis Executed sample FFmpeg workloads, studied scaling benchmarks, and compared cost-performance across EC2 instance families.\nWeek 4: Networking \u0026amp; high-performance compute with C8gn Researched Amazon EC2 C8gn from Blog 2. Learned how ultra-high bandwidth (~600 Gbps) impacts analytics, virtual appliances, and cluster computing workflows.\nWeek 5: Understanding DynamoDB global tables \u0026amp; strong consistency Studied Multi-Region Strong Consistency (MRSC). Designed exercises on zero-RPO scenarios and resilience architecture.\nWeek 6: Working with Bedrock and GenAI APIs Explored Amazon Bedrock API keys (Blog 2). Tested authentication models, prompt workflows, and multi-language features in Amazon Q in Connect.\nWeek 7: Amazon Nova Canvas \u0026amp; virtual try-on Learned about virtual try-on generation, image pipelines, and style models for product visualization. Conducted experiments with 3D animation and vector-style generation.\nWeek 8: SageMaker JumpStart \u0026amp; OpenSearch RAG optimization Followed RAG optimization guide from Blog 2. Deployed sample RAG architecture and tuned OpenSearch latency.\nWeek 9: Serverless distributed SQL (Aurora DSQL) Investigated Aurora DSQL regional expansion and serverless cluster performance. Compared distributed SQL patterns for multi-region workloads.\nWeek 10: Operational insights with Amazon Q Developer Plugins Studied Blog 3 in depth. Tested Datadog and Wiz plugin behaviors using sample workloads. Learned intent recognition, API invocation flow, guardrails, and plugin credential configurations.\nWeek 11: Security, monitoring, and issue triage workflows Practiced queries such as:\n@datadog list my current monitors @wiz list issues with critical severity\nSimulated operational troubleshooting scenarios. Week 12: Final consolidation \u0026amp; real-world cloud operations Reviewed all concepts, connected weekly learning outcomes, and prepared final documentation and architecture summaries for the internship.\nThis worklog summarizes the complete journey—capturing both foundational cloud skills and hands-on practice with new-generation AWS technologies.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event2/","title":"Event Participation – AWS DevOps Bootcamp","tags":[],"description":"","content":"Event 2 – AWS DevOps Bootcamp Event Name: AWS DevOps Bootcamp\nDate \u0026amp; Time: 8:30 AM – 5:00 PM, November 17, 2025\nLocation: AWS Vietnam Office\nRole: Attendee (Student)\nDescription of Event Activities This full-day event delivered a comprehensive, in-depth experience across the core pillars of DevOps on AWS, combining theory, demos, and real-world best practices.\nMorning Session (8:30 AM – 12:00 PM) The day began with a warm welcome and a recap of the previous AI/ML session. The trainers introduced the DevOps mindset, emphasizing culture, collaboration, automation, and continuous improvement.\nTopics included:\nDevOps principles and practices Benefits and performance metrics such as DORA, MTTR, deployment frequency The next major segment focused on AWS DevOps CI/CD services, including:\nAWS CodeCommit and Git strategies (GitFlow, Trunk-based Development) Build \u0026amp; Test automation with AWS CodeBuild Deployment strategies using CodeDeploy (Blue/Green, Canary, Rolling) CI/CD orchestration using AWS CodePipeline Live demo of a complete CI/CD pipeline Following a short break, the session continued with Infrastructure as Code (IaC):\nCloudFormation templates, stack management, drift detection AWS CDK constructs, reusable patterns, multi-language support Demo deploying infrastructure with CloudFormation \u0026amp; CDK Discussion on choosing the right IaC tool for different scenarios Afternoon Session (1:00 PM – 5:00 PM) The afternoon introduced AWS container services, covering:\nDocker fundamentals and microservices containerization Amazon ECR image management, scanning, and lifecycle policies Amazon ECS \u0026amp; EKS orchestration, scaling, and deployment patterns AWS App Runner for simplified container deployment Demo \u0026amp; case study comparing microservices deployment options After another break, the group explored Monitoring \u0026amp; Observability:\nCloudWatch logs, metrics, alarms, dashboards AWS X-Ray for distributed tracing End-to-end demo of full-stack observability setup Best practices for alerting, dashboards, and on-call processes The final segment highlighted DevOps best practices:\nFeature flags, A/B testing, deployment safety techniques Automated testing and CI/CD integration Incident management workflows and postmortems Case studies from startups and enterprise DevOps transformations The day concluded with a Q\u0026amp;A session discussing DevOps career pathways and AWS certification roadmap.\nParticipation \u0026amp; Role As an attendee, I listened attentively, observed demos, and followed the practical explanations of DevOps workflows on AWS. Although not directly hands-on, the event gave me a complete view of modern cloud DevOps practices.\nOutcomes \u0026amp; Value Gained From this event, I gained:\nA deep understanding of DevOps culture, best practices, and the importance of continuous improvement Clear insights into CI/CD workflows using CodeCommit, CodeBuild, CodeDeploy, and CodePipeline Practical knowledge of Infrastructure as Code with CloudFormation and CDK Solid foundational understanding of container services (Docker, ECR, ECS, EKS, App Runner) Knowledge of monitoring \u0026amp; observability using CloudWatch and X-Ray Awareness of deployment strategies, incident management, and postmortems Broader career orientation toward Cloud DevOps and AWS certification paths This event significantly strengthened my understanding of DevOps and reinforced my interest in cloud engineering and automation.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/2-proposal/","title":"Proposal","tags":[],"description":"","content":"CloudRead – Online Ebook Reading Platform A Java + AWS–based Solution for Online Reading and Ebook Management 1. Executive Summary CloudRead is an academic project developed by a team of five Information Technology students, aiming to build an online platform for reading and managing digital books (ebooks) on Amazon Web Services (AWS).\nUsers can register accounts, search for books, and read ebooks directly through an integrated PDF viewer, while also receiving notifications and password recovery emails via Amazon Simple Email Service (SES).\nThe system is developed using Spring Boot (Java 17) for the backend and HTML/CSS/JavaScript (Vanilla JS) for the frontend, fully deployed across AWS services including EC2, RDS, S3, CloudFront, SES, IAM, CloudWatch, and CodePipeline.\nThe project’s objective is to create a stable, scalable, secure, and low-cost academic platform for online ebook reading.\n2. Problem Statement Current Issue\nMost free ebook websites such as nhasachmienphi.com only allow users to download or view content online, lacking essential features such as user role management, reading statistics, or content evaluation.\nCommercial ebook platforms, on the other hand, require high costs and complex configurations, making them unsuitable for small-scale student projects.\nProposed Solution\nCloudRead is designed as an educational and demonstrative platform that enables:\nEbook storage and distribution via Amazon S3 + CloudFront (Signed URL). User, book, and review management through Amazon RDS MySQL. Backend hosting directly on Amazon EC2 (Spring Boot). Static frontend hosting through S3 + CloudFront. Email verification and support via Amazon SES (SMTP). Automated build and deployment using AWS CodePipeline → CodeBuild → CodeDeploy. System monitoring, logging, and cost tracking via Amazon CloudWatch. Benefits \u0026amp; ROI\nProvides practical hands-on experience with real AWS cloud deployment. Operates 24/7 with minimal cost and optimized resource usage. Strengthens technical skills in distributed systems, security, and DevOps practices. Can be expanded in the future with additional learning and sharing features. 3. Solution Architecture CloudRead follows a three-tier architecture integrated with AWS services.\nUsers access the platform through Route 53, with static content delivered by CloudFront from S3 (Frontend Bucket).\nWhen users log in, search, or open an ebook, requests are handled by EC2 (Spring Boot Backend), which retrieves data from RDS MySQL hosted in a private subnet and fetches ebook files from S3 (Private Bucket).\nEmail notifications and support messages are sent via Amazon SES.\nThe backend source code is built and deployed automatically using CodePipeline → CodeBuild → CodeDeploy, and all logs and metrics are tracked through CloudWatch.\nKey AWS Services:\nAmazon EC2 – Runs the Spring Boot backend. Amazon RDS (MySQL) – Stores user, book, and review data. Amazon S3 + CloudFront – Hosts and distributes web assets and ebook files. Amazon SES – Sends user notification and support emails. Amazon IAM – Manages access permissions between EC2, S3, and SES. Amazon CodePipeline / CodeBuild / CodeDeploy – Automates CI/CD workflow. Amazon CloudWatch – Monitors logs, performance, and billing alerts. 4. Technical Implementation Implementation Stages\nLocal Development (1–2 weeks): Design the MySQL database, develop backend APIs, and build the HTML/JS interface with PDF.js. AWS Configuration (2 weeks): Set up EC2, RDS, S3, CloudFront, SES, and IAM roles with secure subnet configuration. CI/CD Integration (1 week): Configure CodePipeline, CodeBuild, and CodeDeploy for automated build and deployment. Testing \u0026amp; Demo (1 week): Validate all features including login, reading, reviewing, and email functions. Technical Requirements\nJava 17 + Spring Boot MySQL (local → RDS) HTML/CSS/JavaScript (Vanilla JS + PDF.js) AWS SDK for Java (S3 + SES integration) GitHub with AWS CodePipeline/CodeBuild/CodeDeploy Postman and CloudWatch for testing and monitoring 5. Timeline \u0026amp; Milestones January: Database design, backend API development, and basic UI. February: Configure AWS RDS, S3, CloudFront, and SES. March: Integrate CI/CD, perform system testing, and demonstrate CloudRead. 6. Estimated Budget The estimated monthly operating cost for CloudRead running continuously is USD 15–17, including EC2 (~6.5), RDS (~7), S3 + CloudFront (~0.7), SES (~0.15), CI/CD (~1), and CloudWatch (~0.1).\nBy leveraging AWS Free Tier and AWS Student Credits ($100), the actual cost during the initial phase can be nearly zero.\n7. Risk Assessment Main risks include AWS connectivity issues, data exposure, and exceeding Free Tier limits.\nMitigation strategies include configuring private S3 buckets with Signed URLs, setting Billing Alarms, verifying SES domain to ensure stable email delivery, and creating periodic EC2 and RDS snapshots.\nIf CI/CD deployment fails, manual rollback will be used to restore a stable version.\nBackup Plans:\nMaintain a local version for offline demo purposes. Regularly monitor logs and system performance via CloudWatch. Control monthly spending through AWS billing alerts. 8. Expected Outcomes Upon completion, CloudRead will be a fully functional online ebook reading and management platform deployed on AWS.\nThe project will equip the team with strong full-stack and DevOps experience, covering system design, cloud deployment, and cost optimization.\nIn the future, CloudRead can be expanded with community-based features, book recommendations, device synchronization, and user authentication through Amazon Cognito.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Strengthen AWS account foundation: security, identity management, and authentication. Learn AWS cost governance: budgeting, usage tracking, and alerts. Understand AWS Support plans and how to work with support tickets. Tasks to be carried out this week: Day Task Start Date Completion Date 8 - Lab01.01 – Create an AWS account - Lab01.02 – Setup Virtual MFA Device 15/09/2025 15/09/2025 9 Lab01.03 – Create admin group and admin user 16/09/2025 16/09/2025 10 Lab01.04 – Account authentication support 17/09/2025 17/09/2025 11 - Lab07.01 – Create Budget by Template - Lab07.02 – Create Cost Budget Tutorial 18/09/2025 18/09/2025 12 - Lab07.03 – Creating a Usage Budget in AWS - Lab07.04 – Creating a Reservation Instance Budget 19/09/2025 19/09/2025 13 - Lab07.05 – Creating a Savings Plans Budget - Lab07.06 – Clean Up Budgets 20/09/2025 20/09/2025 14 Lab09.01 → Lab09.04 – AWS Support Packages \u0026amp; Requests 21/09/2025 21/09/2025 Week 2 Achievements: Successfully created and secured an AWS Root Account, enabling strong governance and ownership.\nCompleted Virtual MFA setup for enhanced authentication security.\nEstablished an IAM administrative structure by creating an admin group with proper permissions and provisioning an admin user following the principle of least privilege.\nGained a deep understanding of AWS account authentication and access management, including:\npassword policies MFA workflows secure credential handling Built strong foundational skills in AWS Cost Management by creating multiple types of budgets:\nCost Budget Usage Budget Reservation Instance Budget Savings Plans Budget Learned how to configure budget alerts using thresholds and notifications to support cost optimization and prevent overspending.\nPracticed the full budget lifecycle: creation → configuration → monitoring → clean-up, ensuring proper financial governance.\nMastered how to work with AWS Support, including understanding support plans, creating support cases, reviewing case history, and navigating the Support Center for troubleshooting and operational assistance.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"AWS Weekly Roundup — The Latest AWS Updates Every Monday, we bring you a summary of the most important AWS announcements of the week. This edition highlights updates across Amazon Bedrock, EC2 C8gn, DynamoDB Global Tables, Aurora DSQL, Amazon Nova Canvas, and more.\nEC2 C8gn — Network Performance up to 200 Gbps Figure 1. EC2 C8gn powered by AWS Graviton4. EC2 C8gn instances are built on AWS Graviton4 and the latest Nitro System, offering:\nUp to 200 Gbps network throughput Significant compute performance gains over previous generations Optimized for workloads such as: High-performance networking Telecom and packet processing Distributed data systems Real-time analytics DynamoDB Global Tables — Multi-Region Strong Consistency AWS introduces Multi-Region Strong Consistency (MRSC) for DynamoDB Global Tables, allowing:\nStrongly consistent reads from any replicated Region Simplified development for global financial and mission-critical applications Zero-RPO data protection guarantees Amazon Nova Canvas — New Creative Styles Amazon Nova Canvas now includes:\nEight new generation styles Enhanced virtual try-on experiences Better outputs for creative and e-commerce use cases Amazon Q in Connect — Now Supports More Languages Amazon Q adds multilingual support across:\nEnglish Spanish French Portuguese Mandarin Japanese Korean Aurora MySQL \u0026amp; RDS MySQL — Integration with SageMaker New integrations simplify ML pipelines:\nNear-real-time data extraction Apache Iceberg table format support Shared data access for analytics engines Aurora DSQL — Now Available in Additional Regions Aurora DSQL expands to:\nAPAC (Seoul) New European Regions Benefits include:\nServerless distributed SQL Global cluster awareness Unlimited horizontal scalability Author Figure 2. Elizabeth Fuentes — AWS. Elizabeth Fuentes\nAWS Developer Advocate\nHelping builders stay ahead with clear, practical AWS insights.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Test the Gateway Endpoint","tags":[],"description":"","content":"Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.3-event3/","title":"Event Participation – AWS Well-Architected Security Pillar Workshop","tags":[],"description":"","content":"Event 3 – AWS Well-Architected Security Pillar Workshop Event Name: AWS Well-Architected Security Pillar Workshop\nDate \u0026amp; Time: 08:30 AM – 12:00 PM, November 29, 2025\nLocation: AWS Vietnam Office\nRole: Attendee (Student)\nDescription of Event Activities This half-day workshop focused on the Security Pillar of the AWS Well-Architected Framework, providing a detailed and structured view of cloud security best practices based on five foundational pillars.\nThe session began with an overview of the Security Pillar and its role within the Well-Architected Framework. Core cloud security principles were introduced, including:\nLeast Privilege Zero Trust Defense in Depth Shared Responsibility Model Common cloud security threats in Vietnam ⭐ Pillar 1: Identity \u0026amp; Access Management (IAM) The workshop’s first pillar explored modern IAM architecture:\nIAM Users, Roles, and Policies Eliminating long-term credentials IAM Identity Center for SSO and centralized permission management SCPs and permission boundaries for multi-account environments MFA, credential rotation, and Access Analyzer Mini demo on IAM Policy validation and access simulation ⭐ Pillar 2: Detection This segment covered continuous monitoring and detection:\nCloudTrail (organization-level logging) GuardDuty for threat detection Security Hub for compliance and posture management VPC Flow Logs, ALB Logs, S3 logs EventBridge for alerting and automation Detection-as-Code concepts ⭐ Pillar 3: Infrastructure Protection Topics included:\nVPC segmentation and network isolation Public vs private subnet placement Security Groups vs Network ACLs AWS WAF, Shield, and Network Firewall EC2, ECS, and EKS workload protection practices ⭐ Pillar 4: Data Protection This section emphasized encryption and secret management:\nAWS KMS: key policies, grants, rotation Encryption at rest \u0026amp; in transit (S3, EBS, RDS, DynamoDB) Secrets Manager \u0026amp; Parameter Store patterns Data classification \u0026amp; guardrails ⭐ Pillar 5: Incident Response The workshop concluded with incident response fundamentals:\nAWS IR lifecycle and playbook structure Handling compromised IAM keys S3 public exposure remediation Malware detection on EC2 Snapshot, isolation, and evidence collection Automated IR using Lambda and Step Functions Participation \u0026amp; Role As a student attendee, I primarily listened to the presentations and followed the explanations and demos. Although not performing hands-on exercises, I gained a comprehensive understanding of cloud security principles and AWS security services.\nOutcomes \u0026amp; Value Gained From this workshop, I gained:\nA clear understanding of the 5 security pillars within the AWS Well-Architected Framework Practical insights into IAM design, detection mechanisms, and workload protection Knowledge of encryption, KMS, secret management, and data lifecycle security A real-world perspective on common cloud security issues in Vietnam Awareness of incident response workflows and automated remediation patterns A stronger security mindset aligned with modern cloud architectures Impact on Career Development This event helped reinforce my direction toward Cloud Security and DevSecOps. The workshop provided clarity on the AWS Security learning roadmap, including paths toward Security Specialty and Solutions Architect Professional, strengthening my long-term career orientation in cloud security.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Understand AWS VPC networking fundamentals. Build hands-on skills in routing, subnets, gateways, and hybrid connectivity. Strengthen network security through Security Groups and Network ACLs. Tasks to be carried out this week: Day Task Start Date Completion Date 15 Module 02-01 – AWS Virtual Private Cloud (VPC) 22/09/2025 22/09/2025 16 Module 02-02 – VPC Security \u0026amp; Multi-VPC features 23/09/2025 23/09/2025 17 Module 02-03 – VPN, DirectConnect, LoadBalancer, Extra Resources 24/09/2025 24/09/2025 18 - Lab03-01 – Start with Amazon VPC \u0026amp; VPN Site-to-Site - Lab03-01.1 – Subnets 25/09/2025 25/09/2025 19 - Lab03-01.2 – Route Table - Lab03-01.3 – Internet Gateway (IGW) 26/09/2025 26/09/2025 20 Lab03-01.4 – NAT Gateway 27/09/2025 27/09/2025 21 - Lab03-02.1 – Security Group - Lab03-02.2 – Network ACLs - Lab03-02.3 – VPC Resource Map 28/09/2025 28/09/2025 Week 3 Achievements: Gained a strong foundational understanding of Amazon VPC, including CIDR concepts, IP addressing, subnets, route propagation, and how VPCs provide secure, isolated environments.\nMastered VPC Security and multi-VPC architecture, including:\nSecurity Groups Network ACLs VPC Peering Transit Gateway cross-VPC communication models Developed knowledge of hybrid connectivity such as Site-to-Site VPN and AWS Direct Connect, and how they integrate on-premises networks with AWS.\nUnderstood the role of Elastic Load Balancing, including traffic distribution, health checks, availability improvements, and integration with VPC layers.\nCompleted hands-on labs building a production-grade VPC:\nCreated public \u0026amp; private subnets Designed and configured Route Tables Attached and configured IGW Deployed NAT Gateway for secure outbound access from private subnets Strengthened network security expertise by configuring Security Groups and NACLs, and validating inbound/outbound traffic behavior.\nCreated a full VPC Resource Map, improving skills in documenting network architecture, dependencies, traffic flows, and security boundaries.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/","title":"Access S3 from VPC","tags":[],"description":"","content":"Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section contains the AWS technical blogs that you have translated. Each blog covers a different topic ranging from computing performance, scalable data architectures, to AI-powered developer tools.\nBlog 1 – Video Encoding on Graviton in 2025 This blog analyzes the performance improvements of video encoding workloads on AWS Graviton processors—especially the newest Graviton4 generation. It explains how optimizations in x265, new SIMD instructions like SVE2, and compiler improvements (Clang vs GCC) result in significantly better throughput and cost efficiency. The blog also provides benchmark data for HDR/10-bit encoding and offers best practices for maximizing performance on Graviton instances.\nBlog 2 – Getting Started with Healthcare Data Lakes Using Microservices This blog describes how a healthcare data lake can be architected using a microservices-based ingestion model. It explains why healthcare data (HL7v2, ER7, clinical records, lab data…) benefits from modular connectors, pub/sub communication, and event-driven processing. The article walks through core components such as S3, DynamoDB, SNS, Step Functions, and API Gateway, showing how to design scalable, maintainable ingestion pipelines.\nBlog 3 – Using Amazon Q Developer with Third-Party Plugins This blog demonstrates how Amazon Q Developer integrates with tools like Datadog and Wiz to enhance observability, monitoring, and security workflows. Through real examples, it shows how developers can query APM data, retrieve cases and alerts, inspect vulnerabilities, and automate operational tasks—all through natural language interactions inside Q Developer. The blog also explains how to configure these plugins in the Q Developer console.\nUse the links above or the sidebar navigation to access each translated blog.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/","title":"Event Participated","tags":[],"description":"","content":"Event 3 – AWS Well-Architected Security Pillar Workshop Event Name: AWS Well-Architected Security Pillar Workshop\nDate \u0026amp; Time: 08:30 AM – 12:00 PM, November 29, 2025\nLocation: AWS Vietnam Office\nRole: Attendee (Student)\nDescription of Event Activities This half-day workshop focused on the Security Pillar of the AWS Well-Architected Framework, providing a detailed and structured view of cloud security best practices based on five foundational pillars.\nThe session began with an overview of the Security Pillar and its role within the Well-Architected Framework. Core cloud security principles were introduced, including:\nLeast Privilege Zero Trust Defense in Depth Shared Responsibility Model Common cloud security threats in Vietnam ⭐ Pillar 1: Identity \u0026amp; Access Management (IAM) The workshop’s first pillar explored modern IAM architecture:\nIAM Users, Roles, and Policies Eliminating long-term credentials IAM Identity Center for SSO and centralized permission management SCPs and permission boundaries for multi-account environments MFA, credential rotation, and Access Analyzer Mini demo on IAM Policy validation and access simulation ⭐ Pillar 2: Detection This segment covered continuous monitoring and detection:\nCloudTrail (organization-level logging) GuardDuty for threat detection Security Hub for compliance and posture management VPC Flow Logs, ALB Logs, S3 logs EventBridge for alerting and automation Detection-as-Code concepts ⭐ Pillar 3: Infrastructure Protection Topics included:\nVPC segmentation and network isolation Public vs private subnet placement Security Groups vs Network ACLs AWS WAF, Shield, and Network Firewall EC2, ECS, and EKS workload protection practices ⭐ Pillar 4: Data Protection This section emphasized encryption and secret management:\nAWS KMS: key policies, grants, rotation Encryption at rest \u0026amp; in transit (S3, EBS, RDS, DynamoDB) Secrets Manager \u0026amp; Parameter Store patterns Data classification \u0026amp; guardrails ⭐ Pillar 5: Incident Response The workshop concluded with incident response fundamentals:\nAWS IR lifecycle and playbook structure Handling compromised IAM keys S3 public exposure remediation Malware detection on EC2 Snapshot, isolation, and evidence collection Automated IR using Lambda and Step Functions Participation \u0026amp; Role As a student attendee, I primarily listened to the presentations and followed the explanations and demos. Although not performing hands-on exercises, I gained a comprehensive understanding of cloud security principles and AWS security services.\nOutcomes \u0026amp; Value Gained From this workshop, I gained:\nA clear understanding of the 5 security pillars within the AWS Well-Architected Framework Practical insights into IAM design, detection mechanisms, and workload protection Knowledge of encryption, KMS, secret management, and data lifecycle security A real-world perspective on common cloud security issues in Vietnam Awareness of incident response workflows and automated remediation patterns A stronger security mindset aligned with modern cloud architectures Impact on Career Development This event helped reinforce my direction toward Cloud Security and DevSecOps. The workshop provided clarity on the AWS Security learning roadmap, including paths toward Security Specialty and Solutions Architect Professional, strengthening my long-term career orientation in cloud security.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Strengthen hands-on skills with VPC creation, subnets, routing, and connectivity. Improve understanding of hybrid DNS and CloudFormation deployments. Build secure and scalable network foundations following AWS best practices. Tasks to be carried out this week: Day Task Start Date Completion Date 22 - Lab03-03 – Create VPC - Lab03-03.2 – Create Subnet 29/09/2025 29/09/2025 23 - Lab03-03.3 – Create an Internet Gateway (IGW) - Lab03-03.4 – Create Route Table for Outbound Internet Routing via IGW 30/09/2025 30/09/2025 24 - Lab03-03.5 – Create Security Groups - Lab03-04.1 – Create EC2 Instances in Subnets 01/10/2025 01/10/2025 25 - Lab03-04.2 – Test Connection - Lab03-04.3 – Create NAT Gateway 02/10/2025 02/10/2025 26 Lab03-04.5 – EC2 Instance Connect Endpoint 03/10/2025 03/10/2025 27 - Lab10-01 – Set up Hybrid DNS with Route 53 Resolver (Introduction) - Lab10-02.1 – Generate Key Pair - Lab10-02.2 – Initialize CloudFormation Template 04/10/2025 04/10/2025 28 - Lab10-02.3 – Configure Security Group - Lab10-03 – Connect to RDGW - Lab10-05 – Set up DNS - Lab10-05.1 → 05.4 – Outbound/Inbound Endpoint, Resolver Rules, Testing - Lab10-06 – Clean Up Resources 05/10/2025 05/10/2025 Week 4 Achievements: Strengthened hands-on proficiency in VPC creation and subnet design, including building a VPC from scratch, defining appropriate CIDR ranges, and creating public/private subnets based on AWS best practices.\nConfigured critical VPC Internet connectivity components, including attaching an Internet Gateway (IGW) and designing Route Tables to correctly route outbound traffic for secure public access.\nImplemented Security Groups suited for subnet roles and deployed EC2 instances into corresponding subnets, ensuring secure segmentation between public-facing and private workloads.\nValidated end-to-end network connectivity, conducted structured connection tests, and deployed a NAT Gateway to allow private subnets to securely access the internet.\nEnhanced operational capability by configuring the EC2 Instance Connect Endpoint, enabling secure, agentless SSH access without requiring public IPs or bastion hosts.\nGained foundational knowledge of Hybrid DNS using Route 53 Resolver, understanding inbound/outbound endpoints and how AWS enables DNS resolution across hybrid networks.\nStrengthened CloudFormation skills, including generating key pairs, initializing templates, and configuring supporting infrastructure such as Security Groups and DNS components.\nCompleted a comprehensive hybrid DNS workflow, including:\nCreating outbound \u0026amp; inbound Resolver Endpoints Configuring Resolver Rules Testing cross-network DNS resolution Cleaning up resources to maintain hygiene and cost efficiency "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn VPC Peering and multi-VPC communication patterns. Strengthen infrastructure provisioning with CloudFormation. Understand Transit Gateway architecture and routing workflows. Tasks to be carried out this week: Day Task Start Date Completion Date 29 Lab19-01 – VPC Peering (Introduction) 06/10/2025 06/10/2025 30 - Lab19-02.1 – Initialize CloudFormation Templates - Lab19-02.2 – Create Security Group - Lab19-02.3 – Create EC2 Instance 07/10/2025 07/10/2025 31 - Lab19-03 – Update Network ACLs (NACLs) - Lab19-04 – Create a Peering Connection 08/10/2025 08/10/2025 32 - Lab19-05 – Configure Route Tables - Lab19-06 – Enable Cross-Peer DNS 09/10/2025 09/10/2025 33 Lab19-07 – Clean Up Resources 10/10/2025 10/10/2025 34 - Lab20-01 – Transit Gateway (Introduction) - Lab20-02 – Preparation Steps 11/10/2025 11/10/2025 35 - Lab20-03 – Create Transit Gateway - Lab20-04 – Create Transit Gateway Attachments - Lab20-05 – Create Transit Gateway Route Tables - Lab20-06 – Add Transit Gateway Routes to VPC Route Tables - Lab20-07 – Clean Up Resources 12/10/2025 12/10/2025 Week 5 Achievements: Gained a deep understanding of VPC Peering, including private connectivity, common use cases, architectural limitations, and related security considerations.\nImproved automation capabilities by initializing CloudFormation templates, creating VPC components at scale, and laying the foundation for consistent infrastructure provisioning workflows.\nConfigured Security Groups and EC2 instances across separate VPCs, preparing environments for controlled cross-VPC communication with accurate inbound/outbound rule design.\nUpdated and hardened Network ACLs (NACLs) to support VPC-to-VPC connectivity while enforcing stateless security boundaries—emphasizing layered network security.\nSuccessfully created and validated VPC Peering Connections, including requester–accepter configuration and connectivity status validation.\nBuilt routing logic for multi-VPC communication by configuring Route Tables and enabling cross-peer DNS resolution, allowing private DNS names to be resolved across VPCs.\nPracticed disciplined environment hygiene by performing complete resource clean-up to maintain cost efficiency and avoid unnecessary infrastructure retention.\nDeveloped foundational understanding of AWS Transit Gateway (TGW), its benefits over VPC Peering in large-scale hub-and-spoke architectures, and its role in simplifying multi-VPC connectivity.\nCompleted hands-on Transit Gateway tasks:\nCreated a Transit Gateway Established VPC Attachments Built and configured TGW Route Tables Propagated and added routes to VPC Route Tables Integrated centralized routing patterns Executed a full Transit Gateway lifecycle, including provisioning, routing setup, connectivity validation, and clean-up to ensure cost and operational efficiency.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-policy/","title":"VPC Endpoint Policies","tags":[],"description":"","content":"When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "},{"uri":"https://thienluhoan.github.io/workshop-template/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":" During my internship at Amazon Web Services (AWS) from September 8, 2025 to December 16, 2025, I had the opportunity to learn, practice, and apply various cloud computing concepts in a real working environment.\nI participated in the First Cloud Journey workshop and contributed to developing the Cloud Read project, which helped me strengthen my skills in Cloud Computing, Solution Architecture, Serverless technologies, and teamwork.\nThroughout the internship, I maintained a strong work ethic, strived for high-quality results, adhered to workplace guidelines, and actively collaborated with team members to improve overall effectiveness.\nTo objectively reflect on my learning journey, I evaluated myself according to the following criteria:\nSelf-Assessment Criteria No. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding AWS/DevOps concepts, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking tasks, not waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time with high quality ✅ ☐ ☐ 5 Discipline Following schedules, rules, and processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve ☐ ✅ ☐ 7 Communication Presenting ideas and reporting clearly ☐ ✅ ☐ 8 Teamwork Cooperation, sharing, and working effectively within the team ✅ ☐ ☐ 9 Professional conduct Respectful attitude, behavior, and workplace etiquette ✅ ☐ ☐ 10 Problem-solving skills Identifying issues, proposing solutions, showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, effort, ideas, and involvement ✅ ☐ ☐ 12 Overall performance Summary of overall ability during the internship ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with rules and workflows of the organization. Improve problem-solving thinking and technical decision-making. Enhance communication skills in both daily interactions and professional contexts, including handling situations more effectively. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Deepen understanding of AWS Compute ecosystem, especially EC2. Learn storage foundations: EBS, Instance Store, backups. Strengthen automation with User Data, Metadata, and Auto Scaling. Gain familiarity with file systems (EFS, FSx) and migration tools (MGN). Tasks to be carried out this week: Day Task Start Date Completion Date 36 - Module 03-01 – Compute VM on AWS - Module 03-01-01 – EC2 Instance Types 13/10/2025 13/10/2025 37 Module 03-01-02 – AMI / Backup / Key Pair 14/10/2025 14/10/2025 38 - Module 03-01-03 – Elastic Block Store (EBS) - Module 03-01-04 – Instance Store 15/10/2025 15/10/2025 39 - Module 03-01-05 – User Data - Module 03-01-06 – Metadata 16/10/2025 16/10/2025 40 Module 03-01-07 – EC2 Auto Scaling 17/10/2025 17/10/2025 41 Module 03-02 – EC2 Autoscaling / EFS / FSx / Lightsail / MGN 18/10/2025 18/10/2025 42 - Lab13-01 – Deploy AWS Backup (Introduction) - Lab13-02.2 – Deploy Infrastructure - Lab13-03 – Create Backup Plan - Lab13-05 – Test Restore - Lab13-06 – Clean Up Resources 19/10/2025 19/10/2025 Week 6 Achievements: Gained a solid understanding of Compute Virtual Machines on AWS, including EC2’s role in the compute ecosystem and how instance lifecycle, provisioning, and management operate in cloud environments.\nMastered the various EC2 Instance Types, including general purpose, compute optimized, memory optimized, storage optimized, and accelerated computing families.\nLearned how to choose optimal instance types for different workloads.\nBuilt strong knowledge of AMI management, including creating custom AMIs, understanding AMI-backed instances, versioning strategies, and using AMIs for backup, scaling, and DR workflows.\nImproved security posture through proper Key Pair management and SSH best practices.\nDeveloped a deep understanding of Elastic Block Store (EBS): volume types, performance, snapshots, encryption, and design best practices for persistent storage.\nDistinguished EBS from Instance Store, recognizing ephemeral storage behaviors and appropriate use cases.\nStrengthened hands-on skills with User Data and Instance Metadata, understanding how EC2 performs bootstrap automation, dynamic configuration, and environment-aware deployment.\nMastered core principles of EC2 Auto Scaling, including launch templates, policies, health checks, lifecycle hooks, and cost-efficient scaling strategies.\nGained exposure to broader compute and storage tools:\nEC2 Autoscaling Elastic File System (EFS) FSx Lightsail AWS MGN Completed full AWS Backup hands-on workflow:\nDeploying backup infrastructure via CloudFormation Creating Backup Plans with rules \u0026amp; lifecycle policies Testing restore operations to validate RTO/RPO targets Cleaning up resources to maintain cost efficiency "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://thienluhoan.github.io/workshop-template/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Feedback This section provides consolidated feedback regarding my performance, learning progress, and overall contribution during the internship at the AWS CloudRead Program.\nMentor Feedback Demonstrates strong enthusiasm and willingness to learn. Able to follow technical guidance and complete assigned tasks with clear effort. Shows improvement in communication and teamwork throughout the project. Growing understanding of AWS, DevOps practices, and cloud architecture fundamentals. Has the potential to advance further with continued commitment and structured learning. Strengths Observed High sense of responsibility and persistence when approaching tasks. Open-minded attitude and readiness to accept feedback for improvement. Good collaboration with team members and positive working spirit. Strong interest in cloud technology, which enhances long-term growth potential. Areas for Improvement Improve consistency and discipline with deadlines and task planning. Strengthen troubleshooting and problem-solving skills for technical tasks. Enhance communication clarity, especially when reporting progress or raising issues. Continue building a strong foundation in AWS Core Services before advancing to complex modules. Overall Evaluation The internship period has shown clear evidence of growth in both soft skills and technical capabilities. With continued effort, structured practice, and consistent learning, strong potential exists for future career paths in DevOps, Cloud Engineering, or Business Analysis.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Deepen understanding of AWS Storage services across object, block, and file storage models. Gain hands-on experience with S3, Backup, Storage Gateway, and hybrid VM migration workflows. Strengthen knowledge of large-scale and hybrid storage architectures. Tasks to be carried out this week: Day Task Start Date Completion Date 43 Module 04-01 – AWS Storage Services 20/10/2025 20/10/2025 44 Module 04-02 – Amazon S3 Access Point – Storage Class 21/10/2025 21/10/2025 45 Module 04-03 – S3 Static Website, CORS, Glacier, Object Key, Performance 22/10/2025 22/10/2025 46 Module 04-04 – Snow Family – Storage Gateway – Backup 23/10/2025 23/10/2025 47 - Lab13-02.1 – Create S3 Bucket - Lab13-02.2 – Deploy Infrastructure - Lab13-03 – Create Backup Plan - Lab13-04 – Set Up Notifications 24/10/2025 24/10/2025 48 - Lab14-01 – VMware Workstation - Lab14-02.1 – Export VM - Lab14-02.2 – Upload VM to AWS - Lab14-02.3 – Import VM to AWS - Lab14-02.4 – Deploy Instance from AMI 25/10/2025 25/10/2025 49 - Lab14-03.1 – Setting up S3 ACL - Lab14-03.2 – Export VM from Instance - Lab14-05 – Cleanup on AWS - Lab24-2.1 – Create Storage Gateway - Lab24-2.2 – Create File Shares - Lab24-2.3 – Mount File Shares - Lab24-3 – Cleanup Resources 26/10/2025 26/10/2025 Week 7 Achievements: Gained a comprehensive understanding of AWS Storage Services, covering object, block, and file storage models, and how AWS designs scalable, durable, and highly available architectures for diverse workloads.\nLearned in depth about Amazon S3, including:\nAccess Points Bucket policies Storage Classes Lifecycle configurations Cost optimization and performance tuning Built strong practical skills with S3 Static Website Hosting, CORS, object key design, and performance optimization techniques such as multipart upload, transfer acceleration, and caching.\nExplored archival storage such as Glacier \u0026amp; Glacier Deep Archive.\nUnderstood the broader AWS hybrid storage ecosystem:\nSnow Family (Snowcone, Snowball, Snowmobile) AWS Storage Gateway Backup and hybrid integration workflows Completed hands-on S3 \u0026amp; Backup labs:\nCreating S3 buckets Deploying infrastructure using CloudFormation Creating Backup Plans with rules, schedules \u0026amp; lifecycle Setting up SNS notifications Gained real-world hybrid virtualization experience through VM migration to AWS, including:\nExporting VMs from VMware Workstation Uploading VM images to S3 Importing VMs into AWS Deploying instances from AMIs Strengthened advanced S3 administration skills by configuring S3 ACL, exporting VM images, and performing environment cleanup for cost hygiene.\nEnhanced hybrid storage capabilities by deploying AWS Storage Gateway, creating and mounting file shares, validating configurations, and performing full cleanup afterward.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Strengthen understanding of AWS Identity, Access, and Security services. Learn IAM, Cognito, Organizations, Identity Center, KMS, and Security Hub. Develop hands-on skills in cloud security governance and compliance monitoring. Tasks to be carried out this week: Day Task Start Date Completion Date 50 Module 05-02 – Amazon Identity and Access Management (IAM) 27/10/2025 27/10/2025 51 Module 05-03 – Amazon Cognito 28/10/2025 28/10/2025 52 Module 05-04 – AWS Organization 29/10/2025 29/10/2025 53 Module 05-05 – AWS Identity Center 30/10/2025 30/10/2025 54 Module 05-06 – Amazon Key Management Service (KMS) 31/10/2025 31/10/2025 55 Module 05-07 – AWS Security Hub 01/11/2025 01/11/2025 56 - Module 05-08 – Hands-on \u0026amp; Additional Research - Lab18-02 – Enable Security Hub - Lab18-03 – Score for each set of criteria - Lab18-04 – Clean up resources 02/11/2025 02/11/2025 Week 8 Achievements: Built a strong foundational understanding of AWS Identity and Access Management (IAM), including IAM Users, Groups, Roles, Policies, Permission Boundaries, and security best practices across AWS environments.\nLearned how Amazon Cognito supports authentication, authorization, and user management for web/mobile applications—covering User Pools, Identity Pools, and identity federation with third-party IdPs.\nGained critical knowledge of AWS Organizations, including multi-account governance, Service Control Policies (SCPs), consolidated billing, and enterprise-grade account structuring strategies.\nUnderstood the purpose and capabilities of AWS Identity Center, including:\nCentralized access management Integration with external IdPs Single Sign-On (SSO) Streamlined permission management across multiple AWS accounts Deepened cloud security expertise through AWS Key Management Service (KMS), including key creation, rotation, key policies, encryption context, and multi-service integration for secure, auditable encryption.\nDeveloped strong monitoring skills using AWS Security Hub, understanding how it aggregates findings, evaluates environments against CIS \u0026amp; AWS security standards, and delivers continuous compliance insights.\nReinforced learning through hands-on lab activities:\nEnabled Security Hub Evaluated security posture via automated scoring Analyzed findings and compliance results Performed full cleanup for cost control \u0026amp; environment hygiene Conducted additional security research, strengthening understanding of identity governance, encryption strategies, threat detection, and cloud security operations.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Strengthen practical experience in automation, tagging, IAM, Lambda, and event-driven operations. Improve governance capabilities across multi-region workloads. Build end-to-end visibility and control through CloudTrail, Athena, and KMS integration. Tasks to be carried out this week: Day Task Start Date Completion Date 57 - Lab22-2.1 – Create VPC - Lab22-2.2 – Create Security Group - Lab22-2.3 – Create EC2 Instance - Lab22-2.4 – Incoming Webhooks Slack 03/11/2025 03/11/2025 58 - Lab22-3 – Create Tag for Instance - Lab22-4 – Create Role for Lambda - Lab22-5.1 – Function Stop Instance - Lab22-5.2 – Function Start Instance - Lab22-6 – Check Result - Lab22-7 – Cleanup 04/11/2025 04/11/2025 59 - Lab27-2.1.1 – Create EC2 Instance with Tag - Lab27-2.1.2 – Managing Tags - Lab27-2.1.3 – Filter Resources by Tag - Lab27-2.2 – Using Tags with CLI - Lab27-3 – Create Resource Group - Lab27-4 – Cleanup 05/11/2025 05/11/2025 60 - Lab28-2.1 – Create IAM User - Lab28-3 – Create IAM Policy - Lab28-4 – Create IAM Role 06/11/2025 06/11/2025 61 - Lab28-5.1 – Switch Roles - Lab28-5.2.1 – Access EC2 in Tokyo - Lab28-5.2.2 – Access EC2 in North Virginia - Lab28-5.2.3 – Create EC2 with missing/qualified tags - Lab28-5.2.4 – Edit Resource Tag - Lab28-5.2.5 – Policy Check - Lab28-6 – Cleanup 07/11/2025 07/11/2025 62 - Lab30-3 – Create Restriction Policy - Lab30-4 – Create Limited IAM User - Lab30-5 – Test IAM User Limits - Lab30-6 – Cleanup 08/11/2025 08/11/2025 63 - Lab33-2.1 – Create Policy \u0026amp; Role - Lab33-2.2 – Create Group \u0026amp; User - Lab33-3 – Create KMS - Lab33-4.1 – Create S3 Bucket - Lab33-4.2 – Upload Data to S3 - Lab33-5.1 – Create CloudTrail - Lab33-5.2 – Logging to CloudTrail - Lab33-5.3 – Create Athena - Lab33-5.4 – Query with Athena - Lab33-6 – Test \u0026amp; Share Encrypted S3 Data - Lab33-7 – Cleanup 09/11/2025 09/11/2025 Week 9 Achievements: Strengthened foundational infrastructure skills by creating a VPC, Security Group, and EC2 instance, reinforcing cloud networking and secure deployment principles.\nIntegrated cloud automation into communication workflows by configuring Slack Incoming Webhooks, enabling automated alerts — a key DevOps monitoring capability.\nEnhanced tagging and automation proficiency through:\nCreating and managing instance tags Creating Lambda execution roles Writing Lambda functions to start/stop EC2 instances Validating automation results Cleaning up resources to maintain a tidy environment Developed strong resource governance skills with AWS Tags:\nCreating, editing, filtering, and managing tags at scale Using tags via the CLI Creating Resource Groups for structured cloud organization Reinforced IAM expertise by creating IAM Users, Policies, and Roles, following least-privilege principles to secure identity workflows.\nGained real-world IAM scenario experience:\nSwitching roles across accounts Accessing EC2 in Tokyo and North Virginia Testing policy behavior with missing/restricted tags Editing resource tags Running policy simulations Performing complete environment cleanup Strengthened security governance by creating Restriction Policies, limited-permission IAM users, and validating enforcement of access controls.\nMastered an end-to-end security visibility and data protection pipeline by completing advanced labs:\nCreating IAM policies, roles, groups, and users Deploying and managing KMS keys Creating S3 buckets and uploading encrypted data Enabling CloudTrail for API auditing Querying CloudTrail logs via Athena Testing secure data sharing with KMS-encrypted S3 objects Cleaning up all resources for cost efficiency "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Strengthen database fundamentals across relational, non-relational, and cloud-native architectures. Build hands-on experience with RDS, Aurora, Redshift, and ElastiCache. Practice real-world database deployment, backup, restore, and migration workflows. Tasks to be carried out this week: Day Task Start Date Completion Date 64 - Module 06-01 – Database Concepts Review 10/11/2025 10/11/2025 65 - Module 06-02 – Amazon RDS \u0026amp; Amazon Aurora 11/11/2025 11/11/2025 66 - Module 06-03 – Redshift \u0026amp; ElastiCache 12/11/2025 12/11/2025 67 - Lab05-2.1 – Create VPC - Lab05-2.2 – Create EC2 Security Group - Lab05-2.3 – Create RDS Security Group - Lab05-2.4 – Create DB Subnet Group 13/11/2025 13/11/2025 68 - Lab05-3 – Create EC2 Instance - Lab05-4 – Create RDS Database Instance 14/11/2025 14/11/2025 69 - Lab05-5 – Application Deployment - Lab05-6 – Backup \u0026amp; Restore 15/11/2025 15/11/2025 70 - Lab05-7 – Clean Up Resources - Lab43-01 → Lab43-06 (EC2 Connect, Oracle/MSSQL Config) - Lab43-07 → Lab43-17 (Schema Conversion, Migration Tasks, Events, Logs, Troubleshooting) 16/11/2025 16/11/2025 Week 10 Achievements: Strengthened database fundamentals through a structured review of relational vs. non-relational models, normalization, indexing, transactions, isolation levels, and high availability concepts essential to cloud database design.\nGained deep knowledge of Amazon RDS and Amazon Aurora, including:\nEngine families Storage models Multi-AZ deployment Read Replicas Automated backups Failover operations Aurora’s distributed storage built for performance and durability Expanded analytical and caching capabilities by learning:\nAmazon Redshift for data warehousing Amazon ElastiCache (Redis/Memcached) for in-memory caching and low-latency workloads Completed foundational database infrastructure labs:\nBuilt a VPC designed specifically for database environments Configured EC2 \u0026amp; RDS Security Groups with least-privilege rules Created a DB Subnet Group spanning multiple Availability Zones Successfully deployed an EC2 instance and created an RDS database instance, forming a complete application-to-database architecture inside a secure network.\nPerformed an end-to-end application deployment connecting EC2 → RDS.\nPracticed essential database operations including backup and restore, verifying durability and recoverability.\nExecuted responsible cloud hygiene through full resource cleanup.\nImproved database migration expertise with an advanced, multi-stage lab involving:\nEC2 Connect for database administration Oracle \u0026amp; MSSQL configuration Schema conversion using AWS SCT Migration tasks using AWS DMS Event monitoring, log analysis, performance tuning, troubleshooting Gained practical, real-world experience in full database migration pipelines, including schema transformation, data replication, validation, and operational cleanup — reflecting authentic enterprise scenarios.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Strengthen data engineering skills using S3, Kinesis Firehose, Glue, Athena, and QuickSight. Build and analyze NoSQL workloads using Amazon DynamoDB. Gain hands-on experience with serverless and event-driven architectures. Tasks to be carried out this week: Day Task Start Date Completion Date 71 - Lab35-3.1 – Create S3 Bucket - Lab35-3.2 – Creating a Delivery Stream 17/11/2025 17/11/2025 72 - Lab35-3.3 – Create Sample Data - Lab35-4.1 – Create Glue Crawler - Lab35-4.2 – Data Check 18/11/2025 18/11/2025 73 - Lab35-5.1 – Explain Code (VN version) - Lab35-5.2 – S3 Store Output - Lab35-5.3 – Session Connect Setup 19/11/2025 19/11/2025 74 - Lab35-6.1 – Analysis with Athena - Lab35-6.2 – Visualize with QuickSight - Lab35-7 – Clean Up Resources 20/11/2025 20/11/2025 75 - Lab39-1 – Hands-on DynamoDB - Lab39-2 – Explore DynamoDB - Lab39-3 – Explore DynamoDB Console 21/11/2025 21/11/2025 76 - Lab39-4 – Back Up - Lab39-5 – Clean Up - Lab39-6 – Advanced Design Patterns for DynamoDB 22/11/2025 22/11/2025 77 - Lab39-7 – Build \u0026amp; Deploy Global Serverless with DynamoDB - Lab39-8 – Serverless Event-Driven Architecture with DynamoDB 23/11/2025 23/11/2025 Week 11 Achievements: Strengthened data engineering fundamentals by creating an S3 bucket and building a Kinesis Data Firehose delivery stream, gaining practical experience in real-time ingestion and durable storage pipelines.\nDeveloped practical skills in sample data generation, ingestion workflows, and downstream analytics.\nBuilt a Glue Crawler to classify data and populate the Glue Data Catalog, then validated schema accuracy through structured data checks.\nEnhanced understanding of ETL workflows by explaining lab code logic, configuring S3 output storage, and establishing proper session connectivity for analytics tasks.\nPerformed end-to-end analytics using Amazon Athena:\nQuerying data directly from S3 Optimizing query performance Validating insights from processed datasets Built visual dashboards using Amazon QuickSight to strengthen BI and visualization capabilities.\nPracticed responsible cloud operations by executing full resource cleanup.\nGained both foundational and advanced Amazon DynamoDB knowledge, including:\nTable creation Data exploration \u0026amp; CRUD operations NoSQL design considerations and workloads Completed DynamoDB backup and restore operations and learned advanced design patterns:\nSingle-table design Partition key optimization Scalability, cost efficiency, and performance tuning Built and deployed global serverless architectures using DynamoDB Global Tables, achieving multi-region replication and high availability.\nExtended serverless knowledge by integrating DynamoDB Streams with other AWS components, building reactive and scalable event-driven architectures.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Strengthen end-to-end database and analytics workflows. Develop multi-interface AWS operational skills. Complete a full modern data pipeline from ingestion to BI dashboarding. Tasks to be carried out this week: Day Task Start Date Completion Date 78 - Lab40-2.1 – Preparing the Database - Lab40-2.2 – Building a Database 24/11/2025 24/11/2025 79 - Lab40-3.1 – Data in the Table - Lab40-3.2 – Cost - Lab40-3.3 – Tagging \u0026amp; Cost Allocation 25/11/2025 25/11/2025 80 - Lab40-3.4 – Usage - Lab40-3.5 – Additional Result Query - Lab40-4 – Clean Up Resources 26/11/2025 26/11/2025 81 - Lab60-1 – CloudShell - Lab60-2 – Console - Lab60-3 – SDK 27/11/2025 27/11/2025 82 - Lab70-1.1 – Create Cloud9 Instance - Lab70-1.2 – Download Dataset - Lab70-1.3 – Upload Dataset to S3 28/11/2025 28/11/2025 83 - Lab70-2.1 – Setting Up DataBrew - Lab70-2.2 – Data Profiling - Lab70-2.3 – Clean \u0026amp; Transform Data 29/11/2025 29/11/2025 84 - Lab72-2 → 72-13 – Full Data Lifecycle - Lab73-3 → 73-5 – Build, Improve \u0026amp; Create Interactive Dashboard 30/11/2025 30/11/2025 Week 12 Achievements: Strengthened database operational skills by preparing and building a cloud-hosted database, designing table structures, validating relational schemas, and inserting sample data to ensure correctness and integrity.\nDeveloped practical cloud financial awareness through hands-on cost analysis—understanding how storage, compute, and queries influence overall cloud spending.\nPracticed tagging and cost allocation to improve tracking and accountability across environments.\nPerformed additional usage analysis and advanced query operations to extract meaningful insights before executing complete environment cleanup.\nGained multi-interface AWS experience through:\nCloudShell (command line) AWS Console (graphical interface) AWS SDK (programmatic access)\nStrengthened ability to manage cloud resources across different operational layers. Built a working Cloud9 development environment, downloaded datasets, and uploaded them into Amazon S3—forming the foundation for data transformation and analytics.\nEnhanced data engineering capabilities with AWS Glue DataBrew:\nData Profiling Data Cleaning Data Transformation Completed a comprehensive, end-to-end modern data lifecycle through Lab72, covering:\nIngest \u0026amp; Store\nCollected and stored raw data in Amazon S3 Catalog\nClassified and indexed data using AWS Glue Transform\nInteractive Glue Jobs GUI-based Glue Jobs DataBrew transformations EMR distributed data processing Analyze\nQueried data with Athena Performed streaming analytics using Kinesis Data Analytics Visualize\nBuilt dashboards and insights with Amazon QuickSight Serve\nDelivered processed data through AWS Lambda Warehouse\nLoaded and organized data into Amazon Redshift Dashboarding\nBuilt, improved, and enhanced interactive BI dashboards Completed multiple dashboard iterations, strengthening skills in:\nVisualization design Insight communication Dashboard interactivity\nfor real-world analytical reporting. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.1-workshop-overview/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Introduction\u0026rdquo; date: \u0026ldquo;2025-09-11\u0026rdquo; weight : 1 chapter : false pre : \u0026quot; 5.1. \u0026quot; VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.2-prerequiste/","title":"","tags":[],"description":"","content":"title : \u0026ldquo;Prerequiste\u0026rdquo; date: \u0026ldquo;2025-09-11\u0026rdquo; weight : 2 chapter : false pre : \u0026quot; 5.2. \u0026quot; IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "},{"uri":"https://thienluhoan.github.io/workshop-template/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://thienluhoan.github.io/workshop-template/tags/","title":"Tags","tags":[],"description":"","content":""}]